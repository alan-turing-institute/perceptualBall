{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Classifiers using Adversarial Perturbations on the Perceptual Ball\n",
    "Andrew Elliott, Stephen Law and Chris Russell \n",
    "\n",
    "This notebook gives a simple example of running our explainability method on a single image.\n",
    "\n",
    "Please note that without a GPU this notebook may take a little time to generate the resultant images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "# Image libraries and image processing functions\n",
    "import torchvision\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# Our method code\n",
    "from common_code.mul_perceptualFunc_Final import *\n",
    "from common_code import utils\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the model, this can be changed to other VGG based variants, however the layers selected later would have to be changed to match the ReLUs in that network. To use with other networks a small change would be required in the feature extractor so it output the correct layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple function wrapper\n",
    "def perceptual(image_name,k=100,model=None,layerLists=['16', '19', '22', '25', '29', '32'],sa=1,ga=10000):\n",
    "    '''\n",
    "    perceptual function that returns a saliency map. \n",
    "    k = iterations\n",
    "    layerLists = the layers to regularise with the perceptual loss\n",
    "    ga = weight of perceptual loss \n",
    "    '''\n",
    "    \n",
    "    # If the model is not specified assume vgg19\n",
    "    if model==None:\n",
    "        model = torchvision.models.vgg19_bn(pretrained=True)\n",
    "        model.requires_grad=False\n",
    "        model.eval()\n",
    "    \n",
    "    # load the image\n",
    "    img_tensor = utils.open_and_preprocess(image_name)\n",
    "    img_variable = img_tensor.unsqueeze_(0)\n",
    "\n",
    "    # create the perceptual loss with the required parameters\n",
    "    loss=create_perceptual_loss2(-2,img_variable,model,gamma=ga,scalar=sa,layers=layerLists)\n",
    "\n",
    "    # optimise the loss to find the adv. perturbation\n",
    "    c=find_direction(loss,img_variable,iterations=k)\n",
    " \n",
    "    # Take pixelwise euclidean distance to get the saliency map\n",
    "    res=torch.sqrt(((c - img_variable)**2).mean(1))\n",
    "    res=res.squeeze().cpu().detach().numpy()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will demonstrate the method on the image in Fig. 2 in the paper. First lets specify our model, we will use a standard VGG19bn pretrained model from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the relevant model\n",
    "premodel = torchvision.models.vgg19_bn(pretrained=True)\n",
    "premodel.requires_grad=False\n",
    "premodel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the layers of the network that correspond to ReLus so we can regularise the correct layers. Note we include "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layers list for VGG19_BN\n",
    "layerAll=['2','5','9','12','16','19','22','25','29','32','35','38','42','45','48','51']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets specify our image. As a demonstration we will use the image from figure in our paper which we display below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image name\n",
    "img='ILSVRC2012_val_00000051.JPEG'\n",
    "im = utils.open_and_resize(img)\n",
    "\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "ax.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets explore the saliency map regularizing on all layers, on CPU this may take a little while to run. To run on GPU, both the model and the image need to be on GPU above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layers to regularise\n",
    "layerslist=[\"0-1-2-3-4-5-6-7-8-9-10-11-12\"]\n",
    "\n",
    "# get layers\n",
    "layers=[layerAll[int(k)] for k in layerslist[0].split('-')]\n",
    "\n",
    "# run adversarial perturbation on the perceptual ball\n",
    "res = perceptual(img,k=100,model=premodel,layerLists=layers,sa=1,ga=10000)\n",
    "\n",
    "# gaussian blur on image\n",
    "mat1 = gaussian_filter(res, sigma=2)\n",
    "\n",
    "# visualise the resultant saliency map\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "ax.imshow(mat1.squeeze())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run this on all of the layer sets in the paper.\n",
    "\n",
    "We first make all of the images for each collection of layers. Note that this might take a while, expecially on CPU.\n",
    "\n",
    "To make this runable by a wide audience we have reduced the number of LBFGS iterations and pushed the computation to CPU, which might result in slightly different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layerslists = []\n",
    "layerslists.append(\"\")\n",
    "layerslists.append(\"0-1-2\")\n",
    "layerslists.append(\"0-1-2-3-4\")\n",
    "layerslists.append(\"0-1-2-3-4-5-6\")\n",
    "layerslists.append(\"0-1-2-3-4-5-6-7-8-9-10-11-12\")\n",
    "\n",
    "images = []\n",
    "for layerslist in layerslists:\n",
    "    if len(layerslist)>0:\n",
    "        layers = [layerAll[int(k)] for k in layerslist.split('-')]\n",
    "    else:\n",
    "        layers = []\n",
    "    res = perceptual(img,k=100,model=premodel,layerLists=layers,sa=1,ga=10000)\n",
    "\n",
    "    # gaussian blur on image\n",
    "    mat1 = gaussian_filter(res, sigma=2)\n",
    "    images.append(mat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resultant perturbations to obtain the similar plot to the version in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,6,figsize=(18,3))\n",
    "titles = ['Orig']\n",
    "titles.append(\"NoPerceptual\")\n",
    "titles.append(\"0-2\")\n",
    "titles.append(\"0-4\")\n",
    "titles.append(\"0-6\")\n",
    "titles.append(\"0-12\")\n",
    "\n",
    "for ax,curIm,title in zip(axs,[im,]+images,titles):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(curIm)\n",
    "    ax.set_title(title,fontsize=20)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
